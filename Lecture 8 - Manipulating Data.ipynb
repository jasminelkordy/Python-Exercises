{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating Data\n",
    "\n",
    "Son Huynh\n",
    "31.01.2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Documentation:\n",
    "- Working with date: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n",
    "- Datetime Index: https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DatetimeIndex.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content:\n",
    "* [1. Data Cleaning](#1.-Data-Cleaning)\n",
    "* [2. Conditional transformation](#2.-Conditional-Transformation)\n",
    "* [3. Time Series transformation](#3.-Time-series-Transformation)\n",
    "* [4. Group transformation](#4.-Group-Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guideline on how to approach data cleaning and validating: https://www.kaggle.com/sohier/structured-eda-for-data-cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl = pd.read_csv('AAPL.csv')\n",
    "\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains some missing values\n",
    "aapl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommended: use pd.to_datetime() and declare the input format explicitly\n",
    "aapl.date = pd.to_datetime(aapl.date, format='%Y/%m/%d', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format convention can be seen here:\n",
    "\n",
    "https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can use `aapl.date.astype('datetime64')` as well, but `pd.to_datetime()` is faster, more robust and you can specify the format and error handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.close = aapl.close.str.replace('$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.close = pd.to_numeric(aapl.close, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `.astype(float)` here as well. However, the advantage of `pd.to_numeric()` is that you can specify `errors='coerce'`, which will convert all non-number / missing values (NaN) to `NaN` for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl[aapl.date.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl[aapl.isnull().any(axis=1)] # select rows with missing data in any column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl[aapl.isnull().all(axis=1)] # only rows with missing data in all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows with missing data in one or many columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = aapl.dropna(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = aapl.dropna(thresh=3) # only keep rows with data in at least 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill missing data with some value\n",
    "\n",
    "There is no best way to fill or drop missing data. Sometimes missing data contain information, thus it is not okay to just drop them. There are multiple approaches to fill and impute missing data. Usually an analyst have to apply logics and knowledge of the data to choose the appropriate method.\n",
    "\n",
    "https://towardsdatascience.com/missing-values-dont-drop-them-f01b1d8ff557\n",
    "\n",
    "https://medium.com/ibm-data-science-experience/missing-data-conundrum-exploration-and-imputation-techniques-9f40abe0fd87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aapl = aapl.fillna(0) # Fill missing values with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use last available value to fill. Sort_index before this step is necessary.\n",
    "aapl = aapl.set_index('date').sort_index()\n",
    "aapl = aapl.fillna(method='ffill')\n",
    "\n",
    "# You can fill the entire dataframe, or choose one column to apply fillna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The high and low columns on April 21 have been filled with data from April 20\n",
    "\n",
    "aapl.loc['2015-04-20':'2015-04-22']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting outliers by descriptive statistic and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.describe() # Some anomaly in max close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.close.plot(kind='line') # Some price is too high, must be errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl[(aapl.close>300)] # These are the rows to be corrected. Looks like they were multiplied by 100 by mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conditional Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### np.where() is like for-loop + if-else but vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's correct the outliers previously spotted.\n",
    "aapl.close = np.where((aapl.close > 300), aapl.close/100, aapl.close)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The logic explained:\n",
    "\n",
    "if aapl.close > 300:\n",
    "    aapl.close = aapl.close/100\n",
    "else:\n",
    "    aapl.close = aapl.close\n",
    "\n",
    "\n",
    "You can add more elif conditions e.g. \n",
    "\n",
    "np.where(condition1, x1, \n",
    "        np.where(condition2, x2, \n",
    "            np.where(condition3, x3, ...)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.close.plot(kind='line') # Now it looks more correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using np.where() to create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl['direction'] = np.where((aapl.close > aapl.open), 'up',\n",
    "                                np.where((aapl.close < aapl.open), 'down', 'unchanged'))\n",
    "\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning with pd.cut()\n",
    "\n",
    "Turn continuous data into categories. Useful for things like converting age to age groups.\n",
    "\n",
    "Check out also pd.qcut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 25000000, 50000000, 100000000, 200000000]\n",
    "labels = ['low', 'medium', 'high', 'very high']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl['volume_cat'] = pd.cut(x=aapl.volume, bins=bins, labels=labels)\n",
    "\n",
    "aapl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time series Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### diff()\n",
    "Also check out `shift()` and `pct_change()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl['profit'] = aapl.close.diff(periods=1)\n",
    "\n",
    "aapl.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day of week\n",
    "\n",
    "Datetime index has some special methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl['day_of_week'] = aapl.index.day_name()\n",
    "\n",
    "aapl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample\n",
    "Resample is just like groupby, but for datetime index.\n",
    "You can use resample if you have a datetime index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl.resample('M').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl.resample('Y').mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Resample\n",
    "\n",
    "D - Day\n",
    "W - Week\n",
    "W-Wed - Week, starting from Wednesday\n",
    "M - Month\n",
    "Q - Quarter\n",
    "Y - Year\n",
    "\n",
    "\n",
    "Return a resampler object, which is similar to a Group by object and has similar methods (sum, mean, count ....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Structure Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack() and unstack()\n",
    "Stack/unstack is very useful when you want to transform the structure of your dataframe. Check out `transpose()` also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stacked = aapl.stack()\n",
    "stacked # returns a series with 2-levels index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstacked = stacked.unstack(level=0) # you can specify which level in the index you want to turn into columns.\n",
    "unstacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl.transpose().stack() # Try playing with transpose and stack/unstack to understand them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot_table()\n",
    "Pivot is great for quick view report on grouped data. It is like a 2 dimension groupby operation. The default groupby function is mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aapl.pivot_table(index='day_of_week', columns='volume_cat', values='close', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
